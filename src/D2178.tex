% !TeX program = luatex
% !TEX encoding = UTF-8


\RequirePackage{luatex85}


\documentclass{wg21}

\usepackage{tikz}
\usepackage[twitter]{emoji}

\usepackage[pdf]{graphviz}



\title{Misc lexing and string handling improvements}
\docnumber{P2178R0}
\audience{EWG, SG-16}
\author{Corentin Jabot}{corentin.jabot@gmail.com}

\begin{document}
\maketitle

\paperquote{}

\section{Abstract}

This Omnibus paper proposes a series of small improvements to the lexing of C++ and the forming of string handling,
notably to clarify the behavior of Unicode encoded sources files, and to reduce implementation divergence.
This paper intends to supersede \paper{N3463} and \paper{P1854R0} and proposes a resolution to several core issues.

While some of these changes are unrelated, the intend of this paper's authors and SG-16 is to rewrite the description of lexing
using more accurate terminology, a wording will, therefore, be provided incorporating all the desired design changes.

Updating both the design an the terminology lets us acknowledge and handle the subtilities of text which may not have been fully understood
in a pre-Unicode world.
The overarching goals are to reduce the divergence in implementations, the complexity of the mental model, and most importantly to make sure that
the semantic of text elements is conserved through the different phases of compilation.

\section{Revisions}

\subsection{R1}
\begin{itemize}
\item Add a note about trailing whitespaces in raw string literals following a comment from the BSI
\end{itemize}

\section{Proposals}

All the proposed changes relate to phases 1-6 of translations.
In particular, this proposal has no library impact.

\subsection{Proposal 1: Mandating support for UTF-8 encoded source files in phase 1}

The set of source file character sets is implementation-defined, which makes writing portable C++ code impossible.
We proposed to mandate that C++ compilers must accept UTF-8 as an input format. Both to increase portability and
to ensure that Unicode related features (ex \paper{P1949R3} can be used widely.
This would also allow us to better specify how Unicode encoded files are handled.

How the source file encoding is detected, and which other input formats are accepted would remain implementation-defined.

Most C++ compilers ( GCC, EDG, MSVC, Clang) support utf8 as one of their input format - Clang only supports utf8.

Unlike \paper{N3463}, we do not intend to mandate or say anything about BOMs, following Unicode recommendations.
BOMs should be ignored (but using BOMs as a means to detect UTF-8 files is a valid implementation strategy, which is notably used by MSVC).

We do \textbf{not} propose to make UTF-8 source file a mandated default, nor the only supported format. Just that there must be
some implementation-defined mechanism (such as a compiler flag) that would tell the compiler to read the file as utf-8.


\subsection{Proposal 2: What is a whitespace or a new-line?}

We propose to specify that the following code point sequences are line-terminators (after phase 1):

\begin{codeblock}
LF:    Line Feed, U+000A
VT:    Vertical Tab, U+000B
FF:    Form Feed, U+000C
CR:    Carriage Return, U+000D
CR+LF: CR (U+000D) followed by LF (U+000A)
NEL:   Next Line, U+0085
LS:    Line Separator, U+2028
PS:    Paragraph Separator, U+2029
\end{codeblock}

Line terminators and the following characters constitute whitespaces

\begin{codeblock}
U+0009 HORIZONTAL TAB
U+0020 SPACE
U+200E LEFT-TO-RIGHT MARK
U+200F RIGHT-TO-LEFT MARK 
\end{codeblock} 

These correspond to characters with the \textbf{Pattern_Whitespace} Unicode property.
The line terminator subset is derived from \textbf{UAX14 - UNICODE LINE BREAKING ALGORITHM}.

We intend to fix \paper{CWG1655} following this clarification.

\subsection{Proposal 3: Preserve Normalization forms}

We propose to specify that Unicode encodes files are not normalized in phase 1 or phase 5, as to preserve the integrity 
of string literals when both the source and the literal associated character set are the Unicode character set.
Instead, the exact sequence of code points of these literals is preserved.
In effect, this does not change the existing behavior of tested implementations in phase 1 (and phase 5 is already specified on a per code point basis).

\pagebreak

\subsection{Proposal 4: Making trailing whitespaces non-significant}

There is a divergence of implementation in how compilers handle spaces between a backslash and the end of a line. 

\begin{colorblock}
int main() {
    int i = 1
    //  \textbackslash  
    + 42
    ;
    return i;
}
\end{colorblock}

EDG(tested with icc front-end) GCC and Clang will trimm the whitespaces after the backslash - and return \tcode{1} - MSVC will not and return \tcode{43}.
Both strategies are valid as part of phase 1 "implementation-defined mapping".
There is a clang issue about this \href{https://bugs.llvm.org/show_bug.cgi?id=15509}{\#15509}

To avoid this surprising implementation divergence we proposed that an implementation must trim all trailing whitespaces before handling \textbackslash \ slicing.
This is reinforced by the fact that IDES and tools may discard such whitespaces. The Google-style guidelines forbid trailing whitespaces.

An additional or alternative approach is to deprecate \textbackslash \ that are not part of a preprocessor directive.
We are not proposing this at this time.

\textbf{Unlike other proposals in this paper, this maybe is a silent breaking change for code that is only compiled with MSVC.}
A quick analysis of the VCPKG package didn't find any trailing whitespaces after backslashes

We have not been able to measure the impact of this proposed change in the MSVC ecosystem. Other compilers, 
and all code supported by these compilers would be unaffected.


\textbf{However, raw string literals should preserve the exact sequence of trailing whitespaces after line splicing reversal}.
The reversal of line splicing in raw strings might not be sufficiently specified, leading to some implementation divergence
and open GCC issues \href{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91412}{\#91412}, \href{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=43606}{\#43606}



\subsection{Proposal 5: Restricting multi-characters literals to members of the Basic Latin Block}

\tcode{int i = 'é';} 
can be equivalent to either \tcode{int i = '\textbackslash u00e9';} or \tcode{int i = 'e\textbackslash u0301';}
depending on source encoding and normalization.
There is also a lot of divergence of implementations in how literals are interpreted. 

\begin{center}
    \begin{tabular}{ c c | c | c | c | c }
        \ & Clang & GCC UTF-8 & MSVC UTF-8 & GCC Latin1 & MSVC latin 1\\ 
        \tcode{'\textbf{e}\textbackslash u0301';} & ill-formed & \tcode{int(0x65CC81)} + Warning & \tcode{int(0x65cc81)} & ill-formed & \tcode{int (0x653f)}\\
        \tcode{'\textbackslash u00e9';} & ill-formed & \tcode{int(0xC3A9)} + Warning & \tcode{int(0xC3A9)} & \tcode{0xFFFFFFFFFFFFFFE9}& \tcode{int(0x09)} \\    
    \end{tabular}
\end{center}

Note the presence of two code points in the first line.

We propose to limit multi-character literals to a sequence of code points from the Unicode Basic Latin Block (\tilde ASCII) to limit the current confusion.

(We do not propose to deprecate multi-character literals).

With the proposed change:

\begin{colorblock}
'c'   // OK
'abc' // OK, multi-characters literal
'\u0080' // OK (if representable in the execution encoding)
'\u0080\u0080' // ill-formed
'é' // OK (if representable in the execution encoding) if one code point (nfc,  {U+00e9}), otherwise (e\textbackslash u0301) ill-formed

\end{colorblock}



\subsection{Proposal 6: Making wide characters literals containing multiple or unrepresentable c-char ill-formed}

The following constructs are well-formed but have widely different interpretations depending on implementations

\begin{colorblock}
wchar_t a = L'@\emoji{1F574}@';
wchar_t b = L'ab';
wchar_t c = L'é';
\end{colorblock}


\begin{itemize}
\item the size of \tcode{wchar_t} being implementation defined, \tcode{L'\emoji{1F574}'} is correctly interpreted on Unix platforms where that size is 32 bits, but truncated by MSVC and other compatible windows compilers where \tcode{wchar_t} is 16 bits.
MSVC first converts to UTF-16, and truncate to the first code unit which results in an invalid lone high surrogate \tcode{0xd83d}.


\item  \tcode{L'ab'} is equivalent to \tcode{L'a'} on msvc and  \tcode{L'b'} on GCC and Clang. All implementation emit a warning under different warning levels 

\item \tcode{L'é'} can be either 1 or 2 c-char depending on the source normalization: \tcode{L'\textbackslash u00e9'} behaves expectedly on all platforms, 
while  \tcode{L'e\textbackslash u0301'} will be \tcode{e} in MSVC and \tcode{U+0301} in GCC AND clang.

\end{itemize}

As such, we propose to make wide characters literals with multiple-chars or char which are not representable in the execution character set ill-formed.

Note that wide characters literals with multiple c-char, unlike multi-character-literals are represented by a single \tcode{wchar_t}.
The other difference is that Unicode combining characters may be representable by a \tcode{wchar_t} whereas they cannot be represented by a \tcode{char}. 
(Note: the first Unicode combining characters appear in the \emph{Combining Diacritical Marks} block, starting at \tcode{U+0300}).


\subsection{Proposal 7: Making conversion of character and string literals to execution and wide execution encoding ill-formed for unrepresentable c-char}

Implementations diverge on how they handle unrepresentable code points when conversion to execution encodings.
GCC and Clang make the conversion ill-formed while MSVC usually replaces unrepresentable characters by a single question mark \tcode{?}.
Strings are text which carries intent and meaning; An implementation should not be able to alter that meaning.

We proposed to make such conversion ill-formed rather than implementation-defined.

After discussions, in SG-16, we do not propose to allow implementations to consider multi-code points graphemes clusters
when doing that conversion. For example considering \tcode{"e\textbackslash u0301"}, \tcode{U+301} does not have a representation in latin 1,
but the abstract character é does (U+00e9 maps to 0x00E9 in ISO/IEC 8859-1).

However, it does not seems possible to guarantee that an implementation knows about all such mapping, which would lead to further implementation
divergence and unnecessary burden on compilers.
We, therefore, propose to be explicit about the conversion being done on each code point independently as is currently the case.

\subsection{Proposal 8: Enforcing the formation of universal escape sequences in phase 2 and 4}

EDG(icc), GCC, MSVC and Clang form universal character names from the following codes:
\begin{colorblock}
'\\
u00e9';
//---
#define CONCAT(x,y) x\#\#y
CONCAT(\, U0001F431);
\end{colorblock}

However, these behaviors are currently UB within the standard.
As such, we propose to make both these behaviors well-defined to follow existing practices. 


\subsection{Proposal 9: Reaffirming Unicode as the character set of the internal representation}

The standard already specifies that characters outside of the basic source character set are converted to UCNs whose values are isomorphic to
Unicode.
We want to make it clear that characters that do not have representation in Unicode are ill-formed.
This includes some characters in some Big5 encodings and exotic languages such as Klingon.

In particular, all characters in EBCDIC
\footnote{For EBCDIC, the mapping of control characters is specified in \href{http://www.unicode.org/reports/tr16/tr16-8.html}{Unicode Technical Report 16 - UTF-EBCDIC}.
This mapping is not semantic-preserving, to the extent control characters have semantics.
}
, GB 18030 have a unique mapping in Unicode.
The intent is to avoid the use of unassigned code points or the Private Use Area by the implementers, as well as to preserve semantic meaning in phase 1.
The preservation of semantic meaning would also make \textbf{invalid utf-8 sequences ill-formed in phase 1}, and other decoding errors (as there exist no mapping for such invalid sequence).
(Note that octal/hexadecimal escape sequences can be used in string literal to form arbitrary binary data)


Notably, it is important to consider that the current specification limits the implementation-defined mapping to universal character names to  valid (0-10FFF) code points,
and any such valid code point can appear in the source before phase 1.
It is therefore not possible for an implementation to uniquely map unrepresentable characters to a valid code point.

This proposal has no bearing on the actual internal representation strategy of already conforming implementations. Notably, mandating internal as-if Unicode representation doesn't
preclude bytewise preservation of narrow and wide string literals when the execution encoding is identical to the source encoding,  as long as there exists a Unicode representation,
as this is otherwise non-observable: It is an important implementation strategy for encoding which are not roundtrip-able through Unicode such as Shift JIS 
to preserve the byte content of string literals when both source and execution encodings are identical.
Such preservation is otherwise non-observable and doesn't need to be mandated, but it needs not to be precluded.
\textbf{We only seek to mandate \emph{representability} in Unicode}. Such representability is notably necessary for the concatenation of literals with different encoding which may happen in phase 6 (\tcode{"foo" u"bar" // u"foobar"}).
Again, this constraint exists in practice as many implementations use Unicode internally and do not use the "implementation-defined mapping" leeway
to nefarious ends.


Specifically:
\begin{itemize}
    \item EBCDIC encodings would be converted to Unicode according to UTF-EBCDIC / IBM CDRA - as such, EBCDIC specific control characters are mapped to C1 control characters. C1 controls characters have no meaning on their own and are designed to be interpreted in an application-specific manner.
    \item GB 18030 maps to Unicode, but a handful of code points maps to the Private Use Area
    \item Big 5: Most abstract characters map to Unicode, with the rare exception of some spelling of some people or place names. Notably, Unicode prescribes a mapping for Windows implementation (code page 951/950)
    \item All other encodings have a complete, semantic preserving mapping to assigned Unicode code points.
\end{itemize}

That list does not intend to be prescriptive, but to show that the C++ standard doesn't need and shouldn't try to handle characters not representable of Unicode.
Furthermore, the author hopes that the wording can convert \emph{universal-character-names} to Unicode code points as soon as they are encountered or formed (which is exclusively a matter of wording,
that would neither affect behavior nor prescribe an internal representation). 

Following this clarification we hope to fix \paper{CWG1332}


\subsection{Proposal 10: Make \tcode{L} in \tcode{_Pragma} ill-formed}

\tcode{_Pragma(L"")} is equivalent to \tcode{_Pragma("")}.

We propose to remove the \tcode{_Pragma(L"")} syntax as both strings are interpreted as sequences of Unicode code points and never as a wide execution literal.
C++ handling of text is confusing enough not to add meaningless characters. This would resolve \paper{CWG897}.
Note that there is a divergence of implementation between C++ and C where C discard all prefixes and C++ only discards L.
 
Out of the 90 millions lines of code of the 1300+ open source projects available on vcpkg, a single use of that feature was found within clang's lexer test suite, for a total of ~2000 uses of \tcode{_Pragma}.
Similarly, the only uses of \tcode{_Pragma (u8"")}, \tcode{_Pragma (u"")}, \tcode{_Pragma (U"")}, etc were found in Clang's  test suite.


\subsection{Proposal 11: Make character literals in preprocessor conditional behave like they do in C++ expression}

\begin{colorblock}
#if 'A' == '\textbackslash x65'
#endif
if ('A' == 0x65){}
\end{colorblock}

Both conditions are not guaranteed to yield a similar result, as \href{http://eel.is/c++draft/cpp#cond-12}{the value of character literals in preprocessor conditional is not required to be identical to that of character literals in expressions}.

However, a survey of the 1300+ open sources projects available on vcpkg shows that the primary use case for these macros is exactly to detect the
execution encoding at compile time and all compilers available on compiler explorer treat these literals as if they were in the execution encoding.

Notably, a few libraries use that pattern to detect EBCDIC or ASCII execution encoding.
Of the ~50 usages of the pattern, all but one where in C libraries.

While we think there should be a better way to detect encodings in C++ \cite{P1885R2}, there is no reason to deprecate that feature.

Instead, we recommend adopting the standard practice and user expectation of converting these literals to the execution encoding before evaluating them.

This also removes yet another theoretical encoding, which further simplifies the mental model. 

\pagebreak

\subsection{Proposal 12: Phase 6 needs fixing}

String literal concatenation happen \textbf{after} each literal has been converted to its associated encoding in phase 5, and as such the standard is actively encouraging mojibake in the following scenarios:

\begin{colorblock}
    L"" ""
    u8"" ""
    u"" ""
    U"" ""
    "" u""
    "" L""
    "" U""
    "" u8""
\end{colorblock}

While string conversion is definitively useful, the current behavior is not.

Indeed, each fragment may be in a different encoding after concatenation: utf-8 data concatenated to shift-jis data, for example.

We propose 2 possible resolutions:

\begin{itemize}
    \item The program is ill-formed unless each \emph{string-literal} part of a concatenation has the same prefix
    
    \item The encoding of the first  \emph{string-literal} determines the encoding of the  \emph{string-literal} resulting from the concatenation and the program is ill-formed if any but the first  \emph{string-literal} literal has an encoding prefix:

\begin{colorblock}
"" ""    // OK, const char*
u8"" ""  // OK, equivalent to u8"" u8"", const char8_t*
u"" ""   // OK, equivalent to u"" u"", const char16_t*
U"" ""   // OK, equivalent to U"" U"", const char32_t*
L"" ""   // OK, equivalent to L"" L"", const wchar_t*
"" L""    // ill-formed
L"" u""   // ill-formed

\end{colorblock}


In which case, each string-literal should be interpreted as having the same prefix in phase 5, so they are converted to the same encoding
prior to concatenation.

\end{itemize}

There is \href{https://godbolt.org/z/-5BTcT}{implementation divergence} in the handling of concatenation:

MSVC converts each string to its associated execution encoding, as specified by phase 6,
while GCC and clang consider the following snippets equivalent (where \tcode{X} is any valid \emph{encoding-prefix})

\begin{colorblock}
    X""  ""
     "" X""
    X"" X""
\end{colorblock}




\pagebreak

\section{Annex: Schematization of text encodings handling during compilation}
\section{Current model}

The graph below is a simplification of the different encodings that appear during compilation.
Each rectangle represents a possibly different encoding.
The red arrows represent operations that may alter the semantic of text elements.

 \digraph{Current}{
    rankdir=TB;
    graph [fontname = "Incosolata"];
    node [fontname = "Incosolata" fontsize=8];
    edge [fontname = "Incosolata" fontsize=7];
    
    src [shape=box, label="Physical Source"]
    internal [shape=box, label="Internal = Basic Source + UCNs"]
    slice [shape=diamond, label="Slice"]
    cpp [shape=diamond, label="Preprocessor"]
    phase5 [shape=diamond, label="Phase 5"]
    
    cppcond  [shape=box, label="Preprocessor Conditionals"]
    e  [shape=box, label="Execution"]
    w  [shape=box, label="Wide Encoding"]
    u  [shape=box, label="UTF-8/16/32"]
    
    src -> internal [label="   Implementation defined mapping" color="red"]
    internal -> slice
    slice -> cpp
    cpp -> phase5 
    
    cpp -> cppcond [color="red"]
    phase5 -> e [color="red"]
    phase5 -> w [color="red"]
    phase5 -> u
}

\section{Proposed model}

In the proposed model, all conversions are either semantic preserving\footnote{The value of multi-character literals remains implementation-defined} or ill-formed. Preprocessor Conditionals use the narrow literal encoding (execution literal encodings).
Outside of raw literals, \emph{universal-character-names} are converted to codepoints as they are formed, the wording is specified in term of the
Unicode character set.

 \digraph{expected}{
    rankdir=TB;
    graph [fontname = "Incosolata"];
    node [fontname = "Incosolata" fontsize=7];
    edge [fontname = "Incosolata" margin=20 fontsize=7];
    
    src [shape=box, label="Abstract Characters Sequence"]
    utf [shape=box, label="UTF-8"]
    internal [shape=box, label="Unicode"]
    cpp [shape=diamond, label="Preprocessor"]
    slice [shape=diamond, label="Trim whitespaces + Slice"]
    phase5 [shape=diamond, label="Phase 5"]
    
    
    e  [shape=box, label="Narrow Literal Encoding"]
    w  [shape=box, label="Wide Literal Encoding"]
    u  [shape=box, label="UTF-8/16/32"]
    
    utf -> internal [label="codepoints preserving" color="blue"]
    src -> internal [label="semantic preserving" color="darkgreen"]
    
    internal -> slice
    slice -> cpp
    cpp -> phase5
    
    phase5 -> e [label="semantic preserving" color="darkgreen"]
    phase5 -> w [label="semantic preserving" color="darkgreen"]
    phase5 -> u [label="codepoints preserving" color="blue"]
}

\section{Acknowledgments}
Thanks to the people who provided feedback on the proposed changes, notably Tom Honnermann, Hubert Tong, Aaron Ballman, Steve Downey.
\section{References}
\renewcommand{\section}[2]{}%
\bibliographystyle{plain}
\bibliography{wg21}

\begin{thebibliography}{9}

    \bibitem[N4861]{N4861}
    Richard Smith
    \emph{Working Draft, Standard for Programming Language C++}\newline
    \url{https://wg21.link/N4861}

\end{thebibliography}
\end{document}
