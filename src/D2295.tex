% !TeX program = luatex
% !TEX encoding = UTF-8


\RequirePackage{luatex85}

\documentclass{wg21}


\title{Support for UTF-8 as a portable source file encoding}
\docnumber{P2295R2}
\audience{SG-16, EWG, SG-22}
\author{Corentin Jabot}{corentin.jabot@gmail.com}

\begin{document}
    
\maketitle

\paperquote{We should have some notion of a portable C++ source file, and without a known fixed encoding it's hard to argue that such a thing exists --- Richard Smith}

\section{Credits}

While many people, notably many people from SG-16 provided feedbacks on the design presented in this paper,
the general direction was originally proposed in 2012 by Beman Dawes in \paper{N3463}.

Thanks Mr. Dawes!

\section{Abstract}

We propose that UTF-8 source files should be supported by all C++ compilers.

\section{Revisions}

\subsection{R2}
\begin{itemize}
    \item Add references for BOM guidelines
    \item Clarify that Clang will support a wider range of source encodings in the future
    \item Clarify that phase 5 codepoint preservation is handled by \paper{P2314R1}.
\end{itemize}

\subsection{R1}
\begin{itemize}
    \item Remove the section about whitespaces and associated wording
\end{itemize}

\section{Motivation}

\begin{quoteblock}
    Even this simple program cannot be written portably in C++ and may fail to compile:
\begin{codeblock}
    int main() {}
\end{codeblock}
 --- Beman Dawes
\end{quoteblock}

The set of source file character sets is implementation-defined, which makes writing portable C++ code impossible.
We propose to mandate that C++ compilers must accept UTF-8 as an input format both to increase portability and
to ensure that Unicode related features (ex \paper{P1949R3}) can be used widely.
This would also allow us to better specify how Unicode encoded files are handled.

How the source file encoding is detected, and which other input formats are accepted would remain implementation-defined.
Supporting UTF-8 would also not require mentioning files in the wording, the media providing the stream of UTF-8 data is not important.

Most C++ compilers (GCC, EDG, MSVC, Clang) support UTF-8 as one of their input format - Clang only supports UTF-8 (but there is an ongoing effort to extend the set of supported
source encodings).

\section{Previous works}

Original Proposal: \paper{N3463} (Beman Dawes, 2012).
More recent discussions about Normalizxation preservation, whitespaces, and UTF in \paper{P2178R1} (Points 1, 2, 3, and 8)

\section{Design}

\subsection{Codepoint preservations}

An important reason to mandate support for UTF-8 support is that it allows us to mandate codepoint preservation during lexing:
the compiler should not try to normalize UTF encoded strings, remove characters, and so forth so that users can expect the content
of strings to be preserved through translation.
(This does not negate that neither the source encoding nor the encoding used internally by the compiler are unobservable by the program).

\subsection{Invalid code units}

Not all code units or code unit sequences represent valid code points.
For example, in UTF-8, some sequences can be overlong (\tcode{0xC0 0xAF}).

In other encodings, some code unit sequences may encode an unassigned code point, which therefore cannot be represented in Unicode.
For example \tcode{0x8 0x00} is an unallocated JIS X 0208:1997 sequence.

In both cases, we believe the program should be ill-formed.
Both these scenarios seldom occur in practice except when the implementation (or the user) infers the wrong encoding.
For example, in \tcode{windows-1251}, \tcode{0xC0} represents the cyrillic letter –ê (U+0410), which when interpreted
as UTF-8 results in an invalid code unit sequence.

As such, the compiler is interpreting the source file incorrectly and the result of the compilation cannot be trusted,
resulting in compilation failures or bad interpretation of string literals further down the line.

The issue can sometimes be innocuous:
frequently, non-ASCII characters appear in comments of files that otherwise only contain ASCII, usually when the name of
a maintainer appears in a source file.
However, most mojibake (encoding miss-interpretation issues) cannot be detected. If the compiler detects invalid characters in comments, there may be undetectable
mojibake in string literals, which would lead to runtime bugs.

Therefore, we think invalid code unit sequences in phase 1 should always be diagnosed.
Our proposed wording makes invalid code unit sequences ill-formed for the UTF-8 encoding.

However, this does not necessarily constitute a breaking change (see the "Impact on implementations" section)

\subsection{A note on unassigned Unicode codepoints}

When mapping from UTF-8 to any representation of a Unicode code point sequence, it is neither necessary to know
whether a codepoint is assigned or not.
The only requirement on codepoints is that they are scalar values.
As such, the proposed change is completely agnostic of the Unicode version.
The only time the Unicode is relevant during lexing is for checking whether an identifier is a valid identifier.

\subsection{BOM}

Unlike \paper{N3463}, we do not intend to mandate or say anything about BOMs (Byte Order Mark), following Unicode recommendations \cite{BOM}.
BOMs should be ignored (but using BOMs as a means to detect UTF-8 files is a valid implementation strategy, which is notably used by MSVC).
Indeed, we should give enough freedom to implementers to handle the cases where a BOM contradicts a compiler flag.
Web browsers for example found a BOM to not be a reliable mechanism.
There are further issues with BOMs and toolings, such that they may be removed by IDEs or source refactoring tools.
They are also widely used, and different companies have different policies.

In any case, mandating BOM recognition goes against Unicode recommendations\footnote{Unicode 13 Standard, and confirmed in a \href{https://corp.unicode.org/mailman/private/unicode/2020-June/008716.html}{mailing list discussion} with members of the Unicode consortium}. But it is a widespread practice, and as such we recommend neither mandating nor precluding any behavior.

\pagebreak

For the purpose of translation, a leading BOM is therefore ignored (and doesn't affect the column count of \tcode{source_location::column()}).
\footnote{Unicode Standard, 23.8: \emph{In those cases, it is not part of the textual content and should be removed before processing, because otherwise it may be mistaken for a legitimate zero width no-break space}.}.

\section{Non goals}

Other existing and upcoming papers try to improve various aspects of lexing related to Unicode, and text encoding, some of them described in \paper{P2178R1}
and \paper{P2194R0}.
This paper only focuses on the handling of UTF-8 physical source files.

In particular, it doesn't try to provide a standard  way to specify or detect encoding for the current translation unit or current source file. This proposal conserves the status quo: the mechanism by which encoded is inferred by the compiler for each source file remains implementation-defined.

We further do not propose to restrict in any way the set of input encodings or "physical source character set"
accepted by compiler beyond mandating that this implementation-defined set contains at least the UTF-8 encoding.

We do not propose a standard mechanism to specify a different encoding per header. This may be explored in a separate paper.

Finally, conservation of code point sequences in phase 5 of translation when encoding a narrow literal character or string in the UTF-8
encoding is not proposed in this paper which focuses on phase 1 of translation.
This is covvered by \paper{P2314R1}.




\section{Impact on implementations}

\subsection{UTF-8 support}
MSVC, EDG, Clang, GCC support compiling UTF-8 source files.

\begin{itemize}
\item Currently (Clang 11), Clang only support UTF-8 and assume all files are UTF-8. BOMs are ignored.
\item GCC supports UTF-8 through iconv and the command line flag \tcode{-finput-charset=UTF-8} can be used to interpret source files as UTF-8.
The default encoding is inferred from the environment and fallbacks to UTF-8 when not possible. BOMs are ignored.
\item MSVC supports UTF-8 source files with the \tcode{/source-charset:UTF-8} command line flag. MSVC uses UTF-8 by default when a BOM is present.
\end{itemize}


\subsection{Input validation}

Compilers currently have very different strategies for handling invalid input:

\begin{itemize}
\item GCC will ensure that a non-UTF-8 input decodes cleanly and will emit an error otherwise. However, when the input is UTF-8 it is
not decoded at all (GCC uses internally) and so the input is not validated. The handling of UTF-8 is then inconsistent with other encodings.
We don't know if this is intentional.
\item Clang does not check invalid comments. By reading the source code this is very intentional. However invalid Unicode is diagnosed (error) in string literals.
\item By default, MSVC will emit a warning for invalid UTF-8, even in comments
\end{itemize}

\begin{quoteblock}
main.cpp(1): warning \href{https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warnings-by-compiler-version?view=msvc-160#warnings-introduced-in-visual-studio-2015-update-2-compiler-version-1900239180}{C4828}: The file contains a character starting at offset 0x2 that is illegal in the current source character set (codepage 65001).
\end{quoteblock}

As such, implementers have two strategies for the implementation of this proposal:

\begin{itemize}
\item Always diagnose invalid code unit sequences when interpreting a UTF-8 input.
\item Provide conforming support for UTF-8 inputs, along with an implementation-defined "UTF-8 like" encoding that would behave like UTF-8 but maybe discard
invalid code units sequences in comments as part of the "implementation-defined mapping" prescribed in phase 1 for non-UTF-8 encodings.
\end{itemize}

And so this proposal guarantees that users can have a way to ensure their source code is properly decoded while giving implementers
the ability to offer more lenient options.

For example, for MSVC, the flags \tcode{/source-charset:utf-8 /we4828} are sufficient to be conforming with the current proposal.


\subsection{UTF-8 source files is existing practice}

\begin{itemize}
    \item By default, VCPKG compiles all the packages in its repository with \tcode{/utf-8} - which sets UTF-8 as the source AND execution encoding (\href{https://github.com/vicroms/vcpkg/blob/master/scripts/toolchains/windows.cmake#L16}{Reference})
    \item Qt source files are UTF-8 - Users of Qt are recommended to use UTF-8 source files (\href{https://wiki.qt.io/Strings_and_encodings_in_Qt}{Reference})
    \item Chromium is built with UTF-8 (by virtue of being compiled with Clang)
\end{itemize}
\pagebreak
\section{SG16 Polls}

See \href{https://github.com/sg16-unicode/sg16-meetings/blob/master/README-2020.md}{Minutes for P2178}

\begin{quoteblock}
It should be implementation-defined whether a UTF-8 BOM is used to inform the encoding of a source file
\\\\    
\begin{tabular}{|c|c|c|c|c|}
\hline
SF & F & N & A & SA \\
\hline
4 & 3 & 1 & 2 & 0 \\
\hline
\end{tabular}

\textbf{Consensus is in favor}
\end{quoteblock}

\begin{quoteblock}
The presence or absence of a BOM is a reasonable portable mechanism
for detecting UTF-8 source file encoding
\\\\
\begin{tabular}{|c|c|c|c|c|}
\hline
SF & F & N & A & SA \\
\hline
0 & 1 & 0 & 3 & 6 \\
\hline
\end{tabular}

\textbf{No consensus};  or rather,
consensus is that a BOM is not a reasonable portable mechanism for detection of source file encoding.    
\end{quoteblock}

\begin{quoteblock}
We agree that, for Unicode source files, that normalization is preserved through translation phases 1 and 5.

\textbf{No objection to unanimous consent.}
    
\end{quoteblock}

\section{Wording}

\subsection{Note on wording}

As SG-16 and Core try to improve the terminology and wording used to describe lexing in the standard,
the proposed wording likely collides with other work happening in parallel.

As such, this wording is based on the current standard.
Once EWG accepts the design, a CWG-targeting paper merging the different lexing related design changes may be brought forward.
As such, the wording reflects the intent but not necessarily the terminology that SG-16 wants to use, nor is it intended to be merged in its current form in the working draft.

Notably, SG-16 and CWG are working on removing the mapping of characters to universal-character-name in phase 1, which has no observable impact on the behavior of C++ programs.

The proposed wording assumes \paper{P2223R1} has previously been applied to the wording draft


\rSec1[lex.phases]{Phases of translation}%

\pnum
\indextext{translation!phases|(}%
The precedence among the syntax rules of translation is specified by the
following phases.

\begin{enumerate}
    \item

\begin{addedblock}
    A UTF-8 source file is a source file encoded as a sequence of UTF-8 code units representing a sequence of UCS scalar values as defined in ISO/IEC 10646.

    \begin{note}
        Invalid UTF-8 code unit sequences in a UTF-8 source file are ill-formed.
    \end{note}
    An implementation accepts UTF-8 source files; The set of additional source file character encodings accepted is implementation-defined.

    How the character encoding of a source file is determined is implementation-defined.
\end{addedblock}

\changed{Physical s}{S}ource file characters are mapped, in an
\impldef{mapping physical source file characters to basic source character set} manner,
to the basic source character set (introducing new-line characters for end-of-line
indicators) \removed{if necessary}.
\removed{The set of physical source file characters accepted is \impldef{physical source file characters}}.
Any source file character not in the basic source character set\iref{lex.charset} is replaced by the \indextext{universal character name}\grammarterm{universal-character-name} that
designates that character.
\added{If the source file is a UTF-8 source file, the scalar value of each source character shall be preserved.}
An implementation may use any internal
encoding, so long as an actual extended character encountered in the
source file, and the same extended character expressed in the source
file as a \grammarterm{universal-character-name} (e.g., using the \tcode{\textbackslash uXXXX} notation), are handled equivalently except where this replacement is reverted\iref{lex.pptoken} in a raw string literal.

\item
\indextext{line splicing}%
\added{If the first codepoint is U+FEFF BYTE ORDER MARK, it is deleted.}
%
Each instance of a backslash character (\textbackslash)
immediately followed by zero or more whitespace characters (other than new-line character) followed by a new-line character is deleted, splicing
physical source lines to form logical source lines. Only the last
backslash on any physical source line shall be eligible for being part
of such a splice.

\end{enumerate}

\section{Acknowledgments}

As usual, many people in SG-16 provided great feedbacks on this paper.
In particular JeanHeyd Meneide offered the great insight that invalid UTF-8 can be supported by being considered a different encoding than UTF-8; Tom Honermann, Jens Maurer and Peter Brett offered valuable feedback!


\section{References}
\renewcommand{\section}[2]{}%
\bibliographystyle{plain}
\bibliography{wg21}

\begin{thebibliography}{9}


\bibitem[Unicode]{Unicode}
Unicode 13\newline
\url{http://www.unicode.org/versions/Unicode13.0.0/}

\bibitem[N4885]{N4885}
Thomas K√∂ppe
\emph{Working Draft, Standard for Programming Language C++}\newline
\url{https://wg21.link/N4885}

\bibitem[UAX-14]{UAX-14}
\emph{UNICODE LINE BREAKING ALGORITHM}\newline
\url{https://www.unicode.org/reports/tr14/}

 \bibitem[UAX-31]{UAX-31}
\emph{UNICODE IDENTIFIER AND PATTERN SYNTAX}\newline
\url{https://www.unicode.org/reports/tr31/}

\bibitem{BOM}
Tom Honermann
Clarify guidance for use of a BOM as a UTF-8 encoding signature
\url{https://www.unicode.org/L2/L2021/21038-bom-guidance.pdf}
    

\end{thebibliography}
\end{document}
