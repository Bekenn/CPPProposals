% !TeX program = luatex
% !TEX encoding = UTF-8


\RequirePackage{luatex85}

\documentclass{wg21}


\title{Support for UTF-8 as a portable source file encoding}
\docnumber{P2295R6}
\audience{SG-16, EWG, SG-22}
\author{Corentin Jabot}{corentin.jabot@gmail.com}
\authortwo{Peter Brett}{pbrett@cadence.com}

\begin{document}

\maketitle

\paperquote{We should have some notion of a portable C++ source file, and without a known fixed encoding it's hard to argue that such a thing exists --- Richard Smith}

\section{Credits}

While many people, notably many people from SG-16 provided feedbacks on the design presented in this paper,
the general direction was originally proposed in 2012 by Beman Dawes in \paper{N3463}.

Thanks Mr. Dawes!

\section{Abstract}

We propose that UTF-8 source files should be supported by all C++ compilers.

\section{Revisions}

\subsection{R6}
\begin{itemize}
    \item Remove a note stating the wording was against \paper{P2314R2} as this paper has been merged.
    \item Rewording using CWG consensus wording.
\end{itemize}

\subsection{R5}
\begin{itemize}
    \item Change wording following SG-16 guidance
\end{itemize}

\subsection{R4}
\begin{itemize}
    \item Improve wording following SG-16 guidance
\end{itemize}

\subsection{R3}
\begin{itemize}
\item Improve wording following SG-16 guidance
\end{itemize}

\subsection{R2}
\begin{itemize}
    \item Add references for BOM guidelines
    \item Clarify that Clang will support a wider range of source encodings in the future
    \item Clarify that phase 5 codepoint preservation is handled by \paper{P2314R1}.
\end{itemize}

\subsection{R1}
\begin{itemize}
    \item Remove the section about whitespaces and associated wording
\end{itemize}

\section{Motivation}

\begin{quoteblock}
    Even this simple program cannot be written portably in C++ and may fail to compile:
\begin{codeblock}
    int main() {}
\end{codeblock}
 --- Beman Dawes
\end{quoteblock}

The set of source file character sets is implementation-defined, which makes writing portable C++ code impossible.
We propose to mandate that C++ compilers must accept UTF-8 as an input format both to increase portability and
to ensure that Unicode related features (ex \paper{P1949R3}) can be used widely.
This would also allow us to better specify how Unicode encoded files are handled.

How the source file encoding is detected, and which other input formats are accepted would remain implementation-defined.
Supporting UTF-8 would also not require mentioning files in the wording, the media providing the stream of UTF-8 data is not important.

Most C++ compilers (GCC, EDG, MSVC, Clang) support UTF-8 as one of their input format - Clang only supports UTF-8 (but there is an ongoing effort to extend the set of supported
source encodings).

\section{Previous works}

Original Proposal: \paper{N3463} (Beman Dawes, 2012).
More recent discussions about Normalization preservation, whitespace, and UTF in \paper{P2178R1} (Points 1, 2, 3, and 8)

\section{Design}

\subsection{Codepoint preservations}

An important reason to mandate support for UTF-8 support is that it allows us to mandate codepoint preservation during lexing:
the compiler should not try to normalize UTF encoded strings, remove characters, and so forth so that users can expect the content
of strings to be preserved through translation.
(This does not negate that neither the source encoding nor the encoding used internally by the compiler are unobservable by the program).

\subsection{Invalid code units}

Not all code units or code unit sequences represent valid code points.
For example, in UTF-8, some sequences can be overlong (\tcode{0xC0 0xAF}).

In other encodings, some code unit sequences may encode an unassigned code point, which therefore cannot be represented in Unicode.
For example \tcode{0x8 0x00} is an unallocated JIS X 0208:1997 sequence.

In both cases, we believe the program should be ill-formed.
Both these scenarios seldom occur in practice except when the implementation (or the user) infers the wrong encoding.
For example, in \tcode{windows-1251}, \tcode{0xC0} represents the cyrillic letter –ê (U+0410), which when interpreted
as UTF-8 results in an invalid code unit sequence.

As such, the compiler is interpreting the source file incorrectly and the result of the compilation cannot be trusted,
resulting in compilation failures or bad interpretation of string literals further down the line.

The issue can sometimes be innocuous:
frequently, non-ASCII characters appear in comments of files that otherwise only contain ASCII, usually when the name of
a maintainer appears in a source file.
However, most mojibake (encoding miss-interpretation issues) cannot be detected. If the compiler detects invalid characters in comments, there may be undetectable
mojibake in string literals, which would lead to runtime bugs.

Therefore, we think invalid code unit sequences in phase 1 should always be diagnosed.
Our proposed wording makes invalid code unit sequences ill-formed for the UTF-8 encoding.

However, this does not necessarily constitute a breaking change (see the "Impact on implementations" section)

\subsection{A note on unassigned Unicode codepoints}

When mapping from UTF-8 to any representation of a Unicode code point sequence, it is neither necessary to know
whether a codepoint is assigned or not.
The only requirement on codepoints is that they are scalar values.
As such, the proposed change is completely agnostic of the Unicode version.
The only time the Unicode is relevant during lexing is for checking whether an identifier is a valid identifier.

\subsection{BOM}

Unlike \paper{N3463}, we do not intend to mandate or say anything about BOMs (Byte Order Mark), following Unicode recommendations \cite{BOM}.
BOMs should be ignored (but using BOMs as a means to detect UTF-8 files is a valid implementation strategy, which is notably used by MSVC).
Indeed, we should give enough freedom to implementers to handle the cases where a BOM contradicts a compiler flag.
Web browsers for example found a BOM to not be a reliable mechanism.
There are further issues with BOMs and toolings, such that they may be removed by IDEs or source refactoring tools.
They are also widely used, and different companies have different policies.

In any case, mandating BOM recognition goes against Unicode recommendations\footnote{Unicode 13 Standard, and confirmed in a \href{https://corp.unicode.org/mailman/private/unicode/2020-June/008716.html}{mailing list discussion} with members of the Unicode consortium}. But it is a widespread practice, and as such we recommend neither mandating nor precluding any behavior.

\pagebreak

For the purpose of translation, a leading BOM is therefore ignored (and doesn't affect the column count of \tcode{source_location::column()}).
\footnote{Unicode Standard, 23.8: \emph{In those cases, it is not part of the textual content and should be removed before processing, because otherwise it may be mistaken for a legitimate zero width no-break space}.}.

\section{Non goals}

Other existing and upcoming papers try to improve various aspects of lexing related to Unicode, and text encoding, some of them described in \paper{P2178R1}
and \paper{P2194R0}.
This paper only focuses on the handling of UTF-8 physical source files.

In particular, it doesn't try to provide a standard  way to specify or detect encoding for the current translation unit or current source file. This proposal conserves the status quo: the mechanism by which encoded is inferred by the compiler for each source file remains implementation-defined.

We further do not propose to restrict in any way the set of input encodings or "physical source character set"
accepted by compiler beyond mandating that this implementation-defined set contains at least the UTF-8 encoding.

We do not propose a standard mechanism to specify a different encoding per header. This may be explored in a separate paper.

Finally, conservation of code point sequences in phase 5 of translation when encoding a narrow literal character or string in the UTF-8
encoding is not proposed in this paper which focuses on phase 1 of translation.
This is covvered by \paper{P2314R1}.




\section{Impact on implementations}

\subsection{UTF-8 support}
MSVC, EDG, Clang, GCC support compiling UTF-8 source files.

\begin{itemize}
\item Currently (Clang 11), Clang only support UTF-8 and assume all files are UTF-8. BOMs are ignored.
\item GCC supports UTF-8 through iconv and the command line flag \tcode{-finput-charset=UTF-8} can be used to interpret source files as UTF-8.
The default encoding is inferred from the environment and fallbacks to UTF-8 when not possible. BOMs are ignored.
\item MSVC supports UTF-8 source files with the \tcode{/source-charset:UTF-8} command line flag. MSVC uses UTF-8 by default when a BOM is present.
\end{itemize}


\subsection{Input validation}

Compilers currently have very different strategies for handling invalid input:

\begin{itemize}
\item GCC will ensure that a non-UTF-8 input decodes cleanly and will emit an error otherwise. However, when the input is UTF-8 it is
not decoded at all (GCC uses internally) and so the input is not validated. The handling of UTF-8 is then inconsistent with other encodings.
We don't know if this is intentional.
\item Clang does not check invalid comments. By reading the source code this is very intentional. However invalid Unicode is diagnosed (error) in string literals.
\item By default, MSVC will emit a warning for invalid UTF-8, even in comments
\end{itemize}

\begin{quoteblock}
main.cpp(1): warning \href{https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warnings-by-compiler-version?view=msvc-160#warnings-introduced-in-visual-studio-2015-update-2-compiler-version-1900239180}{C4828}: The file contains a character starting at offset 0x2 that is illegal in the current source character set (codepage 65001).
\end{quoteblock}

As such, implementers have two strategies for the implementation of this proposal:

\begin{itemize}
\item Always diagnose invalid code unit sequences when interpreting a UTF-8 input.
\item Provide conforming support for UTF-8 inputs, along with an implementation-defined "UTF-8 like" encoding that would behave like UTF-8 but maybe discard
invalid code units sequences in comments as part of the "implementation-defined mapping" prescribed in phase 1 for non-UTF-8 encodings.
\end{itemize}

And so this proposal guarantees that users can have a way to ensure their source code is properly decoded while giving implementers
the ability to offer more lenient options.

For example, for MSVC, the flags \tcode{/source-charset:utf-8 /we4828} are sufficient to be conforming with the current proposal.


\subsection{UTF-8 source files is existing practice}

\begin{itemize}
    \item By default, VCPKG compiles all the packages in its repository with \tcode{/utf-8} - which sets UTF-8 as the source AND execution encoding (\href{https://github.com/vicroms/vcpkg/blob/master/scripts/toolchains/windows.cmake#L16}{Reference})
    \item Qt source files are UTF-8 - Users of Qt are recommended to use UTF-8 source files (\href{https://wiki.qt.io/Strings_and_encodings_in_Qt}{Reference})
    \item Chromium is built with UTF-8 (by virtue of being compiled with Clang)
\end{itemize}
\pagebreak
\section{SG16 Polls}

See \href{https://github.com/sg16-unicode/sg16-meetings/blob/master/README-2020.md}{Minutes for P2178}

\begin{quoteblock}
It should be implementation-defined whether a UTF-8 BOM is used to inform the encoding of a source file
\\\\
\begin{tabular}{|c|c|c|c|c|}
\hline
SF & F & N & A & SA \\
\hline
4 & 3 & 1 & 2 & 0 \\
\hline
\end{tabular}

\textbf{Consensus is in favor}
\end{quoteblock}

\begin{quoteblock}
The presence or absence of a BOM is a reasonable portable mechanism
for detecting UTF-8 source file encoding
\\\\
\begin{tabular}{|c|c|c|c|c|}
\hline
SF & F & N & A & SA \\
\hline
0 & 1 & 0 & 3 & 6 \\
\hline
\end{tabular}

\textbf{No consensus};  or rather,
consensus is that a BOM is not a reasonable portable mechanism for detection of source file encoding.
\end{quoteblock}

\begin{quoteblock}
We agree that, for Unicode source files, that normalization is preserved through translation phases 1 and 5.

\textbf{No objection to unanimous consent.}
\end{quoteblock}

Polls from the 14-04-2021 (R2)

\begin{quoteblock}
We wish to require implementations to support UTF-8 source files.

\textbf{No objection to unanimous consent.}
\end{quoteblock}


\begin{quoteblock}
We wish to require implementations to be capable of accepting UTF-8
source files whether or not they begin with a U+FEFF byte order mark.

\textbf{No objection to unanimous consent.}
\end{quoteblock}

\begin{quoteblock}
We wish to require implementations to have a mode in which they diagnose
ill-formed UTF-8 source files (regardless of whether the ill-formedness is
located in comments, header names or string literals).
\\\\
\begin{tabular}{|c|c|c|c|c|}
    \hline
    SF & F & N & A & SA \\
    \hline
    8 & 2 & 0 & 0 & 0 \\
    \hline
\end{tabular}

\textbf{Very strongly in favour}
\end{quoteblock}

\section{Wording}

\rSec1[lex.phases]{Phases of translation}%

\pnum
\indextext{translation!phases|(}%
The precedence among the syntax rules of translation is specified by the
following phases.


\ednote{The following wording represent the CWG consensus.
While this wording achieve the intent of this paper, the author finds it unfortunate that we use vague terms like "kind of source file" instead of talking about their encoding.
Indeed, by notably keeping the explicit mention of records in the non utf-8 case - a concern that I think should exist outside of the standard, before phase 1 - this wording can be interpreted as utf-8 files stored in records as exempt from the well-formedness and codepoint preserving constraints, which isn't really motivated, and can be the object of further refinements in future papers.}

\begin{enumerate}


\item

\begin{addedblock}
%    A physical source file is a sequence of integers with an associated encoding scheme determined in an
%    implementation-defined manner.
%
%    An implementation shall support the UTF-8 encoding scheme, and may support an implementation-defined set of additional encoding schemes.
%    If encoding schemes other than UTF-8 are supported,
%    an implementation shall provide a means by which the UTF-8 encoding scheme can be specified, independent of the content of the source file. \begin{note} A command-line option that specifies the encoding scheme to use as the result of the determination is such a means.\end{note}
%
%    If the encoding scheme of a physical source file is determined to be UTF-8, then the
%    physical source file shall be a well-formed UTF-8 code unit sequence. The source file is decoded
%    to produce a sequence of UCS scalar values that constitutes the sequence of
%    elements of the translation character set. \begin{note} There are no end-of-line
%    indicators apart from the content of the UTF-8 code unit sequence. \end{note}
%
%    For any other encoding scheme supported by the implementation,

An implementation shall support input files that are a sequence of UTF-8 code units (UTF-8 files). It may also support an implementation-defined set of other kinds of input files, and, if so, the kind of an input file is determined in an implementation-defined manner that includes a means of designating input files as UTF-8 files, independent of their content. \begin{note}In other words, recognizing the U+FEFF Byte Order Mark is not sufficient.\end{note}

If an input file is determined to be a UTF-8 file, then it shall be a well-formed UTF-8 code unit sequence and it is decoded to produce a sequence of UCS scalar values that constitutes the sequence of elements of the translation character set.

For any other kind of input file supported by the implementation, \end{addedblock}
\removed{Physical source file} characters are mapped, in an
implementation-defined manner, to \changed{the translation character set}{a sequence of translation character set elements} (introducing new-line characters for end-of-line
indicators).
\removed{The set of physical source file characters accepted is \impldef{physical source file characters}}.

\item
\indextext{line splicing}%
\added{If the first translation character is U+FEFF BYTE ORDER MARK, it is deleted.}
%
Each instance of a backslash character (\textbackslash)
immediately followed by zero or more whitespace characters (other than new-line character) followed by a new-line character is deleted, splicing
physical source lines to form logical source lines. Only the last
backslash on any physical source line shall be eligible for being part
of such a splice.

\end{enumerate}

%\section{Annex: Wording for paragraph 1 with changes made by P2348 applied}
%
%An implementation shall support physical source files that are a sequence of UTF-8 code units (UTF-8 source files). It may also support an implementation-defined set of other kinds of physical source files, and, if so, the kind of a physical source file is determined in an implementation-defined manner, which includes a means of designating physical source files as UTF-8 source files, independent of their content.
%\begin{note}In other words, recognizing the U+FEFF Byte Order Mark is not sufficient.\end{note}
%
%If a physical source file is determined to be a UTF-8 source file, then it shall be a well-formed UTF-8 code unit sequence and it is decoded to produce a sequence of UCS scalar values that constitutes the sequence of elements of the translation character set.
%
%For any other kind of physical source file supported by the implementation, characters are mapped, in an implementation-defined manner, \changed{to the translation character set (introducing new-line characters for end-of-line indicators)}{to a sequence of translation character set elements}.
%
%
%\section{Annex: Wording for paragraph 1 with changes made by P2348 applied and "physical" replaced by "input"}
%
%\ednote{
%The term "physical source files" is both vaccuous an missleading as source files may not be files at all.
%Ideally we would get rid of the term "source file" in phase 1 entierly, but we need a way to maintain a relation with
%the term "source file" use in the document, both to denotes source files and their content post phase 1.
%As there is preexisting wording specifying that source files are not files, using the term input makes it a bit clearer that
%some amount of processing - such as reading the content of a file/db/record and reconsituting a structure may happen before phase 1.
%The C++ standard should not concern itself with the medium in which source files are stored, rather starting processing from some textual content.
%An alternative would be to replace "physical source file" by "source file" as the distinction is not done in the rest of the document.
%For example "A include preprocessing directive causes the named header or source file" denotes what is in phase one a "physical source file".
%"physical source files" would unfortunately also be a new term, as the status quo wording uses "physical source files characters" only.}
%
%
%An implementation shall support \changed{physical source}{input} files that are a sequence of UTF-8 code units (UTF-8 source files). It may also support an implementation-defined set of other kinds of \changed{physical source}{input} files, and, if so, the kind of \changed{a  physical source}{an input} file is determined in an implementation-defined manner, which includes a means of designating  \changed{physical source}{input} files as UTF-8 source files, independent of their content. \begin{note}In other words, recognizing the U+FEFF Byte Order Mark is not sufficient.\end{note}
%
%If a  \changed{physical source}{input} file is determined to be a UTF-8 source file, then it shall be a well-formed UTF-8 code unit sequence and it is decoded to produce a sequence of UCS scalar values that constitutes the sequence of elements of the translation character set.
%
%For any other kind of  \changed{physical source}{input} file supported by the implementation, characters are mapped, in an implementation-defined manner, to a sequence of translation character set elements.

\rSec2[diff.cpp14.lex]{lexical conventions}

Valid \CppXIV{} code that uses trigraphs may not be valid or may have different
semantics in this revision of \Cpp{}. Implementations may choose to
translate trigraphs as specified in \CppXIV{} if they appear outside of a raw
string literal, as part of the mapping from \changed{physical}{input} characters to
the translation character set.


\section{Acknowledgments}

As usual, many people in SG-16 provided great feedbacks on this paper.
In particular JeanHeyd Meneide offered the great insight that invalid UTF-8 can be supported by being considered a different encoding than UTF-8; Tom Honermann, Jens Maurer offered valuable feedback!


\section{References}
\renewcommand{\section}[2]{}%
\bibliographystyle{plain}
\bibliography{wg21}

\begin{thebibliography}{9}


\bibitem[Unicode]{Unicode}
Unicode 13\newline
\url{http://www.unicode.org/versions/Unicode13.0.0/}

\bibitem[N4901]{N4901}
Thomas K√∂ppe
\emph{Working Draft, Standard for Programming Language C++}\newline
\url{https://wg21.link/N4901}

\bibitem[UAX-14]{UAX-14}
\emph{UNICODE LINE BREAKING ALGORITHM}\newline
\url{https://www.unicode.org/reports/tr14/}

 \bibitem[UAX-31]{UAX-31}
\emph{UNICODE IDENTIFIER AND PATTERN SYNTAX}\newline
\url{https://www.unicode.org/reports/tr31/}

\bibitem{BOM}
Tom Honermann
Clarify guidance for use of a BOM as a UTF-8 encoding signature
\url{https://www.unicode.org/L2/L2021/21038-bom-guidance.pdf}


\end{thebibliography}
\end{document}
